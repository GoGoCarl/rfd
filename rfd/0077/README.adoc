:author: Alex Wilson
:email: alex.wilson@joyent.com
:state: draft
:revremark: State: {state}

:showtitle:
:toc: left
:numbered:
:icons: font

////
    This Source Code Form is subject to the terms of the Mozilla Public
    License, v. 2.0. If a copy of the MPL was not distributed with this
    file, You can obtain one at http://mozilla.org/MPL/2.0/.

    Copyright 2017 Joyent Inc
////

# RFD 77 Hardware-backed per-zone crypto tokens

## Introduction

While most existing access control for Triton and Manta focusses on
authenticating *users* and authorizing their actions, there are quite a
few contexts in which authenticating individual instances (zones) on the system
is useful.

This particularly applies in the context of headnode service zones in Triton,
where it would be desirable for each service to be able to authenticate itself
to any other service, giving a path forward to eliminate the special "trusted"
status of all hosts on the admin VLAN.

This kind of authentication is generally most easily done through cryptography,
which is what this document will propose.

To store cryptographic key material securely with a machine, in a way that is
highly resistant to compromise, the state of the art is to make use of a
segregated hardware device (a "token"). The token stores the key material
internally and will not reveal it to the host (and is generally a physically
tamper-resistant device, making it extremely difficult to recover the key
material from it without destroying it). The host machine may request the token
to take particular actions with the key material, such as signing a given
stream of data, encrypting a block of data, or computing a hash function.

This kind of secured credential storage would be useful not just for
authentication, but also for the protection of data at rest. As a result,
this proposal also includes provisions for supporting this use case (though
implementing the on-disk data encryption is delegated to ZFS).

## Definitions

CN:: A "CN" here is any physical node in the Triton system.
Trusted CN:: A "trusted" CN is one that is trusted to run components of the
Triton system itself (e.g. VMAPI, NAPI, CloudAPI etc)
Self-booting CN:: A "self-booting" CN is one that can boot entirely standalone,
without any other machine in the datacenter or network access being available.
Headnode:: A "headnode" is a term for a CN that is both trusted and
self-booting. Any Triton datacenter needs to have at least one such node.

## Threat models and goals

### Authentication

The core threat models (and containment goals) for the authentication scheme
proposed here are as follows:

Threat 1:: An intruder has escalated their network access into the admin VLAN, by
compromise or poor configuration of network equipment or non-Triton
resources.
Goal 1:: The goal is to give this attacker no access to Triton resources. They
may make a read-only request for a boot image for a new CN which will contain
no special credentials, but no more. They may be able to carry out denial of
service attacks on the admin VLAN, but these are out of scope for this design.

Threat 2:: An intruder has escalated their network access into the admin VLAN, by
compromise of an ordinary (not "trusted") compute node (privilege escalation and
zone escape).
Goal 2:: The goal is to give this attacker only the minimum access required for
the normal operation of the CN. They will be able to control other zones on that
CN, as well as the information reported about them back to the rest of Triton.
They will under no circumstances be able to gain control of a trusted CN from
this position. Their access to the system can be terminated by revoking the
credentials of the CN, they cannot extract any long-lived key material, and
cannot take any actions that would escalate or allow sideways movement into
other CNs.

Threat 3:: An intruder has taken control of a public-facing headnode service
(e.g. CloudAPI), by making use of a vulnerability in that service.
Goal 3:: The goal is to give this attacker only the minimum access required by
the normal operation of that service. This means, for example, that CloudAPI
would not be able to run arbitrary commands on CNs or directly interface with
CN agents, or connect directly to the PostgreSQL database (since such access
is not needed for its normal operation).

### Encryption at rest

For the encryption of data at rest, the primary threat model is as follows:

Threat 1:: An intruder gains physical possession of disks and/or hardware
from a CN, either by post-disposal acquisition ("dumpster diving"), or outright
physical theft.
Goal 1:: The goal is to give the attacker no ability to read any customer data
on the disks or (in the case of a disposed CN) any ability to use the
credentials of the CN to gain access to Triton resources. If a stolen CN is
powered up at the time of theft, it is possible that customer data can be read,
but if powered down, no data access will be possible.

[[customer-features]]
### Customer-facing features

This design also seeks to provide 3 key customer-facing features:

Feature 1:: The ability to use a provisioned instance/zone/VM in a customer
account as an authentication principal to Triton (and other Triton-aware)
services.
Goal 1:: The credentials of this principal should not be able to be permanently
compromised by an attacker who has full control of a customer zone (i.e. they
must not be able to access key material).

Feature 2:: The ability to have customer-provisioned instances authenticate
to each other (both within a datacentre and between them) using credentials
provided by Triton itself.
Goal 2:: The credentials used for this authentication should not be able to be
permanently compromised by an attacker who has full control of a customer zone.

Feature 3:: The ability to implement a secure data store protected by hardware
symmetric keys within a zone.
Goal 3:: If an attacker compromises a customer zone storing N items of data
protected by this mechanism, they should have no choice but to make N individual
round trips through a (rate-limited) hardware module in order to decrypt them.
If the attacker compromises an entire live Triton CN (including the contents of
RAM) with M zones on it, they should have no choice but to make at least M round
trips through a hardware module in order to access customer data so protected.

## Design

The central component of the design is the credential storage device. Since
many components of our threat model and goals are on a per-CN basis, we want a
device that can be deployed with (or ideally, inside) every CN. This implies
that:

 * The device must be inexpensive (at least, relative to expected cost of CN
   hardware);
 * The device must be capable of storing credentials both for at-rest encryption
   and for authentication; and
 * The device must not require invasive modification to current-generation
   x86 server hardware.

Most commonly, cryptographic token devices obey an API similar to PKCS#11, which
is primarily focussed on public/private asymmetric cryptography. Devices that
only implement asymmetric cryptography are suitable for storing authentication
credentials, but do not fit as well in a design that wants to store credentials
for at-rest encryption.

In hardware there are always difficult trade-offs between price, features, and
performance. What is implicit in the above list of goals is that the
cryptographic performance of the device is likely to be low (as it is both
cheap and well-featured). As a result, the rate at which hardware operations
need to take place must to be limited in the system design.

One device that is suited for these goals is the Yubikey (manufactured by
Yubico). It implements a number of features aimed at the 2-factor Authentication
market (based on hash chains and HMAC) which are also ideal for securely
deriving encryption keys. Alongside these features, it features RSA and ECDSA
asymmetric cryptography.

The Yubikey is relatively inexpensive (at $40 US it is a very small line item in
the typical cost of a new CN), and since it uses the ubiquitious USB interface
it can easily be added to existing server hardware (in fact, many servers
include USB connectors that are located inside the server casing which are
ideal locations for this use).

Alternatives to the Yubikey that are also well suited include a few models of
USB JavaCard tokens, such as the Feitian eJava token (also sold as the PIVKey
T800). These tokens can be written with appropriate JavaCard Applets to become a
drop-in replacement for the Yubikey (exposing the same commands to the server).

The hardware details of these devices and the interfaces they expose is
discussed further in the section <<hardware>>.

### Overview: at-rest encryption

The concept for at-rest encryption is to generate a master key for ZFS crypto
by combining 3 pieces of data:

 * A secret key written to the hardware token (which it will not reveal);
 * A secret key stored on a trusted node in the datacenter; and
 * A randomly generated "challenge" value, kept on disk unencrypted with the
   data.

The primitive used to combine these pieces of data is the HMAC (see also
<<crypto-algos>>). First, the challenge value is read in from the disk and
passed to the token. The token will compute the HMAC of the challenge data with
its secret key (without revealing that secret key to the host).

Then, a secret key stored in a headnode service will be retrieved over a
TLS-protected authenticated channel and used as the secret key for another HMAC
operation on the output of the first one.

The final output is the master key to unlock the ZFS crypto framework for the
pool. A single master key will be used for the whole pool, rather than a key
per zone or per customer: in the current Triton design, CNs are the source of
truth about what zones run on them (and changing that here is out of scope), so
there is no real benefit in using a finer-grained scheme.

We incorporate the 3 pieces of data into the key so that the only sufficient
condition to successfully decrypt the data on the disks is to have all 3 of:

 * The disks themselves,
 * The key stored in the CN's Yubikey, and
 * Access to the headnode service.

If any one of these 3 is missing, the key cannot be recomputed, and the data
cannot be decrypted.

This approach has one major issue, however, which is the case of a headnode. A
Triton headnode, as defined earlier, must be able to boot from its own media,
without requiring the rest of the surrounding DC to be running (as it may be
hosting the PXE DHCP server that allows other non-self-booting CNs to boot).

As a result, self-booting nodes will not use a remotely stored part in their
key. They will use a challenge value, and the secret key in their token, but
make no remote request to get a third piece. This also means that self-booting
nodes do not meet the full goal discussed above -- the theft of an entire
working headnode will allow that headnode's disks to be read.

This is a difficult compromise between fault tolerance, ability to boot the
whole DC up after power loss, and security. It may be worthwhile to examine
the possibility of special physical security measures to protect headnodes
beyond those used for ordinary non-headnode CNs. As there is normally a
small number of headnodes, this is at least more feasible than such protections
for the entire server population.

### Overview: authentication

Authentication of a CN to a headnode service (e.g. to join the cluster, and
then to report data about running zones etc) is done by signing existing
protocol units (e.g. HTTP requests) using the asymmetric keys stored in the CN's
Yubikey. This is relatively straightforward.

Authentication of one headnode service zone to another is also done by signing
existing protocol units using asymmetric keys. Unfortunately, hardware tokens
are generally only capable of storing a small number of asymmetric keys, and the
number of zones on a CN or headnode may be quite large by comparison. So the
keys used for zone-to-zone authentication cannot reside directly on the hardware
tokens.

Instead, a "soft token" design will be used. A second HMAC secret stored on the
token is used as an HMAC key, along with an input randomly generated for each
zone, to derive a key used to encrypt a keystore for that zone. This keystore
encryption is always used, so that the same code path is taken on machines
with and without ZFS level storage encryption available.

The encrypted key store is managed by the global zone on behalf of the zones,
and exposed to them via a socket that processes in the zone can connect to. The
non-global zone cannot add or remove keys from the key store; it only holds keys
that the global zone has generated and assigned to it.

The socket is designed to make use of the OpenSSH agent protocol. This protocol
is designed to be simple and straightforward to parse in a secure manner, and
since the SSH agent is more or less a "soft token" itself, an almost perfect
match for this use case.

The SSH agent also features support for SSH certificates, which can be used to
attest about an identity associated with a given key. The CN's global zone will
generate one such certificate for each zone and sign it using the same key it
uses for HTTP signature authentication. In this way, zones each have access to a
signed statement from their host CN about their identity, which they can use as
part of authentication.

A signed statement or certificate and a matching key is not enough on its own,
however, to validate the identity of one zone to another arbitrary zone on the
system -- the other zone needs to also be able to validate the key of the host
CN. To achieve this requires a chain of trust.

### Trusted CNs and chain of trust

As is typical with any chain of trust, we must begin with a set of keys known
as "root keys", which are ultimately trusted. What we propose here is to use
a single root key which is only ever stored offline, broken into pieces.

This root key will sign an initial statement stating that certain nodes in the
cluster are to be Trusted CNs, detailing their public keys, as well as a
timestamp and serial number. It will then (barring exceptional circumstances)
never be used again.

To this statement, the Trusted CNs of the datacenter may append additional
statements, with certain restrictions:

 * Any appended statement must include a signature both over the new statement
   and all previous statements in the chain; and
 * The appended statement must be signed by the keys of all Trusted CNs in the
   datacenter at the time of appending, except one (N-1 out of N, unless there
   is only one Trusted CN at the time, in which case its signature is required).

The statement may declare that a new node (with corresponding key etc) is now
a Trusted CN, or it may declare that an existing Trusted CN is no longer such.

All CNs in the system (both regular and trusted) periodically gossip their
current version of the Trusted CN chain out over the network, to a multicast
address on the admin VLAN.

If a CN receives a new chain, it will accept it as the new canonical version
of the chain if and only if:

 * All signatures on the chain validate, including validation of the N-1/N
   restriction; and
 * The chain is a strict extension of the current canonical chain known to the
   CN; OR
 * The chain is an unrelated brand new chain, with a higher serial number and
   newer timestamp on the very first statement.

In this way, in an emergency situation, the chain can be restarted by using the
offine master key to sign a new statement about the Trusted CNs for the
installation.

This design allows Trusted CNs to be added and removed from the installation at
a later date without requiring that the root of the chain of trust be available
in online storage for signing.

Once the gossip process has stabilized, all CNs in the system are aware of the
identities and keys of nodes that are authorized to act as Trusted CNs (hosting
core Triton services). This means that zone certificates presented by zones on
these CNs can be validated, authenticating headnode services to each other.

It is important to note that changes to the set of Trusted CNs are expected to be
infrequent, so it is not important to use a distributed system here that offers
fast convergence. The simplicity of implementation of a gossip design is also
an advantage.

### TLS keys and Soft HSM

Aside from the main zone authentication key and its matching certificate, the
soft token stores two more keys on behalf of the non-global zone: a TLS
certificate signing key, and a symmetric key.

The TLS certificate signing key can only be used to sign X.509 certificates
about keys generated locally within the zone. A Triton-specific extension to the
SSH agent protocol allows for this, as well as the ability to request a
certificate chain.

The certificate chain consists of a set of X.509 certificates describing,
in order:

 1. A trusted head node in the datacentre (self-signed)
 2. The host CN of the zone (its hardware key, signed by the head node)
 3. The soft-token TLS signing key for the zone (signed by the host CN)

These certificates (both the TLS signing key for the zone and the chain
certificates, other than the head node) are limited to a very short window of
validity (60 seconds). The intention is that this chain can be obtained and used
only during an authentication process, and a fresh certificate obtained
regularly to repeat the operation as neeeded. There is no need to check with a
separate revocation list or manage one, as the short lifetime ensures that the
key in question is vouched for by the system: all that clients are required to
do is to keep their list of head node CA certificates up to date with the state
of the gossip engine.

The symmetric key stored in the soft token is treated differently to other keys
in token storage. It is not kept decrypted in memory in the soft token when not
in use; instead, a round trip through the system's hardware module must be made
for every use of this key. This also implies that access to this key is
rate-limited by the system to avoid users overburdening the hardware module.

Rather than encrypting material directly with this key, a data key scheme is
used. This means that each "encrypt" or "decrypt" request made to use this key
must be accompanied by an encrypted subkey. Inside the soft token, the subkey is
decrypted using the master key, which is then used to encrypt or decrypt the
actual data. This further limits the burden users may impose directly upon the
system's hardware module (by limiting the maximum amount of data that must be
transferred through the token itself).

An encrypted subkey ready for use may be obtained using a third operation
through the token interface. All 3 of these operations (encrypt, decrypt, and
generate subkey) are Triton-specific extensions to the SSH agent protocol.

The intention of the symmetric key capability is to enable the implementation
of systems that achieve the 3rd customer goal in <<customer-features>>.

### Binder and service registration

Having to make use of and validate full certificate chains for all traffic is
somewhat difficult to work into some existing systems within Triton. A simpler
proposition is to include only some form of key signature in these types of
traffic (e.g. by embedding it a legacy username and password) rather than a full
certificate.

To this end, `binder` (the Triton service discovery mechanism) will be altered,
such that clients can establish a trusted relationship with binder, and binder
can then take over the role of validating certificates on clients' behalf.

As the client half this relationship can be maintained from within a library
such as `cueball`, this will ease integration for headnode services -- they will
merely need to use the `cueball` library to manage their connections and will
then get identity validation on their outgoing connections "for free".

On the registration side of binder, registrants will be required to supply their
SSH certificate and public key along with the information they supply to binder
today (which will be signed with the key).

Binder will validate the signature and certificate provided, and then serve
DNS records about the registrant. These records will include public key records
containing the registered public key they supplied.

Traffic between binder and clients will be secured using DNS Transaction
Signatures (TSIG), signed using the binder instance's zone key. The client must
validate the binder instance's key against its certificate and the gossiped list
of Trusted CNs, but thereafter it can trust signed responses from that binder
about other services in lieu of performing full validation itself.

Binder will also have to transition away from using the raw ZooKeeper direct
access for registration that it uses today, as the authentication schemes
available there will not be sufficient to ensure separation of clients.

### Provisioning and backups

When crypto tokens like the Yubikey are manufactured, they generally do not ship
with credentials pre-loaded on them (Yubikeys do in fact ship with some
basic credentials for the Yubico official 2FA, but this is not very useful
for our usecase). They have to be commanded to generate or write credentials
by an administrator who configures them before use.

While credentials like authentication keys are best generated on the token
itself (so that they never leave it and thus cannot be compromised), encryption
keys used to protect data at rest must be managed more carefully.

The loss of at-rest encryption keys leads to the loss of any data protected by
them (this means loss of customer data). As a result, they must be backed up in
some form of secured offline storage -- one classic technique is to print on
archival paper and store in a secured mechanical safe in an environmentally
controlled area.

Keys may be split up into "pieces" for backup purposes, using secret-sharing
arrangements like Shamir's secret sharing. These enable schemes such as N out of
M piece secret recovery (while revealing no information in the case of fewer
pieces being held).

The scheme we propose is as follows:

 * Generation and preparation of the root key and token for the initial set of
   Trusted CNs will take place in an environment away from the data center, and
   will be done in advance by administrators.

 * At the same time, the administrators must initialize backup media that have
   been chosen to store the backed up key pieces. Initializing the media writes
   a private key to the start of the media (or in a file on it with a well-known
   name, depending on the media type) and saves only the public half of this key
   for later use.

 * Token authentication keys will be generated on the token and not backed up.
   The public half of the asymmetric keys will be prepared in a format ready to
   upload directly into Triton command-line and web UI tools, so that they are
   added to the DC's headnode in advance.footnoteref:[not-puppet,Note that this
   procedure ensures administrators are not expected to perform error-prone
   key fingerprint comparisons in the datacenter while setting up servers.]

 * Token encryption keys (HMAC keys) will be generated, written to the token,
   and then split into 3 pieces, in a Shamir arrangement requiring 2 pieces for
   recovery. The pieces will be immediately encrypted within a DH "box"
   with an ephemeral key and a backup media public key (one key per piece) so
   that they can only be recovered with the use of the backup media private key.
   Then they may be transported by any appropriate means to the location of each
   backup media to be written out. They need not be decrypted when writing out
   to the media (as the media private key is there to decrypt them during
   recovery).

This scheme will be implemented as a set of tools that can run on at least OSX,
Linux or SmartOS, to correctly program Yubikeys and back up credentials, either
in bulk, or as part of a pre-flight environment run during deployment. The
choice of a backup option by the administrator will not be optional (as not
doing so may lead to data loss in the case of a single Yubikey malfunction).

A recommended outline of the full deployment procedure is included in the
sections <<green-field>> and <<brown-field>>, which include examples for both
a "small setup" deployment not using a pre-flight environment, and a larger
deployment using one.

The tooling to initialize backup media will ensure that each initialization
operation takes place on different media, and will produce the media public key
in an opaque, checksummed format. The tooling for programming Yubikeys will
refuse to operate unless it is provided with a minimum number of valid backup
media identities in this correct opaque format. This helps prevent
administrators from erroneously failing to back up keys.

The three Shamir pieces must be stored separately on independent backup media,
generally recommended to be either archival paper, or LTO or DAT magnetic tape.
Optical media is the next most reliable option, followed by flash media such as
high quality SD cards.

The following table highlights the recommended options for long-term key backup,
as well as a recommended verification and refresh interval for each.

The verification interval indicates how often (at a minimum) an administrator
should inspect and verify the data on the backup media to check its integrity.
The refresh interval indicates a minimum interval at which administators should
expect to have to copy the data to fresh media. Even if the current media
passes inspection, it is recommended that media older than this still be
replaced.

.Backup media recommendations
[options="header"]
|===

| Media type               | Verification interval | Refresh interval

| Magnetic tape (LTO, DAT) | 5 years               | 10 years

| Printed archival paper   | 3 years               | 10 years

| Optical (CD, DVD, BD)    | 1 year                | 5 years

| Flash (SD, CF)           | 1 year                | 3 years

|===

The initial preparation of the offline root key for a datacenter will be
done using the same tooling as regular key programming and generation, and will
be written out as 3 pieces encrypted to the backup media private keys.

Full tooling will also be provided for recovering from these backup formats
a specified CN encryption key, combining the Shamir pieces, and writing it
to a fresh Yubikey ready for use. This tooling can also be used during
regular media inspections to check data integrity.

### Use of zone keys and certificates by customers

Quite aside from the internal use of zone keys and certificates within Triton's
components, they are also expected to be used by customers.

In conjunction with the RBACv2 work (RFD 48), signing requests to Triton
services (such as CloudAPI) using a zone authentication key will grant
authentication as a "machine principal". This principal may be added to roles by
a customer, in order to grant it authorization to manage resources under the
account.

The `keyId` string used is expected to include the full UUID of the zone in
question, and the UUID of the CN which hosts it. This mechanism will not
require the use of the zone certificate.

Since the existing `triton` tools and libraries already support the use of the
SSH agent for key storage, it is expected that they can be used with the
zone soft token without significant modification (they may require some in
order to generate the `keyId` correctly, but this is as yet unclear).

The existing support for account-key-signed certificates for Docker and CMON
will be extended to support the use of those interfaces as a machine principal,
as well. This mechanism is preferred for customer end-use here rather than the
TLS certificate signing key, as it matches the interface already used elsewhere,
reducing the amount of code needed to be specific to machine authentication.

Though it is somewhat out of scope here, it is expected that mechanisms for
grouping machines as access control targets (e.g. RFD 48 style projects) may
also be useful for grouping machines as principals. In this way it should be
possible to grant some group of machines access to account resources and have
this apply to newly provisioned members of that group automatically.

While zone SSH certificates and certificates signed by the TLS certificate
signing key are not used for Triton authentication, endpoints on CloudAPI will
be added to assist in the validation of zone certificates by customer code or
services. These include fetching the current full set of headnode CA
certificates for the X.509 chain. This should allow zone keys and certificates
to be used for other purposes as well (such as bootstrapping a chain of trust
for customer systems).

In particular, it is expected that full support for this mechanism will be
developed to assist with the bringup of the Hashicorp Vault product. Vault
should hopefully also be able to take advantage of the Soft HSM key system.

## Relationship with TLS

To fully protect the Triton admin VLAN against IP and MAC spoofing attacks from
rogue network hardware, it will be necessary to begin protecting all connections
with TLS. Part of establishing a TLS connection is verifying the identity of
both parties to the connection, using X.509 certificates.

The zone TLS certificate signing key is set aside for this purpose. Headnode
services will generate local keys for use by TLS servers, protected at rest by
the Soft HSM key. A signed certificate and chain will be obtained through the
soft token interface to allow these to be validated to others.

It is the responsibility of any Triton service to ensure that it obtains a
new certificate chain for its TLS server endpoints before the expiry of a
previous chain.

As these certificates have an enforced short lifetime of 60 seconds, no
specific provision for certificate revocation is needed: only a requirement that
the list of valid CA certificates be kept up to date by clients to match the
output of the headnode gossip system.

## Service startup step-by-step

### CloudAPI

 . The Trusted CN hosting the CloudAPI instance boots up (see <<cn-boot>>
   for more details)
 .. It starts up the zone soft token manager daemon, which will LoFS mount
    sockets into all zones (see <<soft-token>>). The daemon does not unlock the
    keystores at startup.
 . The CloudAPI zone begins to start up
 .. Soft token socket is mounted into the zone.
 . SMF service `cloudapi` starts -- it execs `node`
 . CloudAPI calls into the `triton-registrar` library to set up its service
   registration
 .. Registrar opens the soft token socket and retrieves the public key and
    certificate signed by the GZ.
 ... Soft token manager daemon accepts the connection on the socket in the zone
     and forks off a dedicated privilege-separated child for this zone. The
     child then decrypts the keystore and loads it into memory.
 .. Registrar connects to binder zones and begins registration by writing a
    signed statement about the CloudAPI zone's IP address and keys, including
    the SSH certificate signed by its CN.
 .. Binder receives and validates the registration
 ... First, binder retrieves the list of valid Trusted CNs from the gossip service
     on its host CN (via the soft token socket)
 ... Then, it compares the signature on the certificate given by the registrant
     to this list and finds it was signed by a valid Trusted CN
 ... The certificate presented includes metadata about the zone, including any
     values of `sdc_role` or `manta_role` tags. Binder validates that such
     values should be allowed to register under the given DNS name.
 ... After validating the signature on the statement from the registrant, binder
     begins serving DNS records about it.
 . CloudAPI opens its cueball pool to connect to VMAPI
 .. Cueball is running in bootstrap mode, and first establishes a bootstrap
    resolver to connect to binder
 ... The bootstrap requests each binder's certificate by looking up the binder
     service hostname with rrtype CERT (see RFC4398)
 ... The bootstrap resolver then retrieves the list of valid Trusted CNs from the
     gossip service on its host CN, and uses this list to validate the binder
     instances' certificates. It also checks that the `sdc_role`/`manta_role`
     value matches up.
 ... The TSIG information on the response is also validated.
 ... The bootstrap emits only the binders that pass validation (along with their
     keys) to be used as resolvers.
 .. Cueball begins service resolution for VMAPI
 ... It uses the resolvers from the bootstrap stage to contact binder and
     request SRV records for VMAPI (and validates the response's TSIG using the
     keys from the bootstrap).
 ... Validated records are emitted as backends
 .. Cueball connects to VMAPI
 ... TLS is established, and the VMAPI's certificate and chain is validated
     against the known CA certificates (obtained by querying the soft token).
 . Now CloudAPI is registered and connected to VMAPI. It repeats these steps
   (without bootstrap, since that's already done) for other services.
 . When CloudAPI wants to make a request to VMAPI, it takes a pre-validated
   TLS connection from the pool and makes an HTTP request on it.
 .. The outgoing HTTP request is signed with the zone key of CloudAPI, and
    includes CloudAPI's registered binder hostname (the service name) as part
    of the keyId.
 .. VMAPI requests the CERT records associated with the name connecting to it
    from binder and validates that a key there matches the one signing the
    incoming request.
 .. Then, VMAPI validates the connecting service name against its own policy of
    which services are allowed to talk to it, and decides whether to accept or
    reject the request.

[[cn-boot]]
## CN boot

Unlike headnodes, ordinary Triton CNs boot over the network. Today, this is
designed to happen by launching the iPXE binary from flash media within each
server. The iPXE binary then makes a DHCP request, and receives a response
containing an HTTP URI from which to fetch the kernel and `boot_archive`.

iPXE supports HTTPS with certificate validation, and this will be used to secure
the CN boot process. It is currently considered unreasonable to add a full
software stack needed to produce signatures from the Yubikey's asymmetric keys
in iPXE, however, so it is proposed that anonymous access to the kernel image
and `boot_archive` be maintained as it is today (i.e., the authentication
at this stage will be one-way: the CN verifying the boot server's identity,
guarding against rogue DHCP and HTTP servers).

Since iPXE's certificate validation mechanism is limited to a set of CA
certificates, which have to reside on the same flash media as iPXE itself, we
treat boot-up here slightly differently to regular service-to-service (or
CN-to-service) authentication.

On the flash media with iPXE will be a set of self-signed X.509 certificates
describing the keys of each of the headnodes in the datacenter at the time when
the flash media is prepared.

The `booter` zones in the installation will generate a local TLS private key
each, and have it cross-signed by the signing keys of all the headnodes in the
data center. They will serve the full set of cross-signed certs in their TLS
handshake, as alternative chains footnoteref:[alt-chains,"Alternative chains"
here refers to the TLS notion of providing a single entity certificate, signed
by a single issuer DN, and then providing multiple certificates for that issuer
DN that are signed by different upstream issuers themselves. This practice is
already commonly used in the Internet today when introducing new CAs and is
quite widely supported.], so that the flash media need only contain one
headnode in common with the real current set for the boot to be successful.

Once a CN has been set up and is operating normally, it will periodically
mount its boot flash media and update the set of headnode CA certificates stored
there.

Some Triton installations do not boot iPXE from flash media, and instead use the
built-in PXE ROM in their system. Unfortunately, the only known way to build an
authenticated system around the firmware PXE is to leverage the EFI Secure Boot
and TPM features of a modern system, and support for using these with PXE is
difficult (due to lack of general EFI support) and somewhat inconsistent between
server vendors. It would also require the ability to modify at runtime the
certificates stored in firmware for boot signing, which currently is not a
well-supported procedure, regularly subject to vendor firmware bugs and
exclusion.

For this reason, installations which depend on system PXE firmware will not have
a fully secured boot procedure, and will not meet all of the stated goals of the
system. This may be revisited at a later date.

### EFI Secure Boot

No provision is made in this document for the implementation or management of
EFI Secure Boot in Triton. EFI support in illumos is not yet complete, and
several unresolved problems remain before a design can be proposed here.

This will likely be the subject of a future RFD.

[[green-field]]
## Green-field deployment step-by-step

This section will run through the full set of steps needed to deploy Triton
with full RFD 77 security enabled.

We begin the process by setting up the root key on an administrator workstation.
On this workstation, we will begin by burning 3 DVD-Rs on which to store key
backups.

After inserting the first blank DVD-R:

[source,shell]
----
alex@mbp:~$ triton-keymaster init-media dvd <1>
Found blank DVD media in HL-DT-ST DVDRW GX30N RP09 (scsi 1,0,0) <2>
Initialize? [Y/n]
Generating media key... done
Writing session... 10% 25% 50% 75% 100% done
Backup media identity: VEJLTSFMx9IR+nWC7FFnUC8pCTMEZL5iloLlU/xjG8x+z1jax6Xb5dvWOMzerJmkiwaK54GnNeoOLH7++R9BlGHzTAQSDMR35qW60+0PLqNEpRhtDg== <3>
Short name to refer to this media? [214cc7d2] sfo-001 <4>
----
<1> We want to initialize a new DVD type backup media. The name we give here
    refers to the storage plugin to be used.
<2> The plugin detects that we have a blank unused DVD-R in one of our drives.
<3> This string must be kept in order to use this media in future. The
    `triton-keymaster` tool will automatically record it in the current user's
    `~/.triton` directory, as well.
<4> This name will be used with later `triton-keymaster` commands. If we want
    to use this same media from a different machine, we can copy the file
    `~/.triton/keymaster.json` or use `triton-keymaster add-media` and
    the full media identity string.

We perform these same steps for the subsequent 2 DVD-Rs, naming them `ord-001`
and `nyc-001`.

[source,shell]
----
alex@mbp:~$ triton-keymaster init-media dvd -y -n ord-001 <1>
Found blank DVD media in HL-DT-ST DVDRW GX30N RP09 (scsi 1,0,0)
Generating media key... done
Writing session... 10% 25% 50% 75% 100% done
Backup media identity: VEJLTWcqNLRmhEMG5ip91j9UzbQEakCyrLl4SJdv/D+FJo3C+uGhEwapnn7Yf+E+PB7ZiwhUcc1N8xOBDI/z5oc52wG+juwhxwj+tGgUR64N1XUCgg==
alex@mbp:~$ triton-keymaster init-media dvd -y -n nyc-001
Found blank DVD media in HL-DT-ST DVDRW GX30N RP09 (scsi 1,0,0)
Generating media key... done
Writing session... 10% 25% 50% 75% 100% done
Backup media identity: VEJLTTVyDUe4yKRTRY4iZzrEnAgEH4p5yyaqC2jMmNIy8x4lPl3jmbX7fEUxNSNkROAulT25fTJcfDMM/b0dPaXf+u6D4/LnyoQXRNdbNjFSMyjrXQ==
----
<1> `-y` means "don't prompt me for confirmation", and `-n` is used to give the
    media short name.

Now we insert a blank Yubikey into the system and proceed:

[source,shell]
----
alex@mbp:~$ triton-keymaster init-dc us-west-1 -m sfo-001,ord-001,nyc-001 <1>
Generating root key... done
Generating first headnode keys... done
Found Yubikey (Yubikey 4 OTP), serial 4a6f94, v4.3.1
Setting Yubikey to OTP+CCID mode... done
Remove Yubikey from USB port and re-plug now... ok <2>
Found Yubikey (Yubikey 4 OTP+CCID), serial 4a6f94, v4.3.1
Writing first headnode keys to Yubikey... done
Ready to write piece for backup media sfo-001.
Attach where? [LOCAL/remote/file] <3>
Found sfo-001 in HL-DT-ST DVDRW GX30N RP09 (scsi 1,0,0)
Writing session... 10% 25% 50% 75% 100% done
Ready to write piece for backup media ord-001.
Attach where? [LOCAL/remote/file]
Found ord-001 in HL-DT-ST DVDRW GX30N RP09 (scsi 1,0,0)
Writing session... 10% 25% 50% 75% 100% done
Ready to write piece for backup media nyc-001.
Attach where? [LOCAL/remote/file]
Found nyc-001 in HL-DT-ST DVDRW GX30N RP09 (scsi 1,0,0)
Writing session... 10% 25% 50% 75% 100% done
----
<1> The `-m` option allows you to supply the names of the backup media keys to
    use for this datacenter. If not supplied, you will be prompted.
<2> The Yubikey has to be physically removed from the USB port at this point to
    change its mode. When this step is done by a pre-flight environment, it
    uses a full system cold reboot instead.
<3> After the initial media setup, backup media can be accessed in multiple
    different ways by the `keymaster` tool. They can be attached locally to
    the machine it is being run on (as shown here), or attached to a remote
    machine (with `keymaster` also installed), or written to a file to be
    transferred later. The key backups are encrypted in transit and cannot be
    read without the backup media itself.

In our initial visit to the datacenter, we have decided we would like to deploy
our single headnode and 3 ordinary CNs. We've already written the Yubikey for
the headnode (during the `init-dc` step above), so now we need to write 3
ordinary CN Yubikeys.

[source,shell]
----
alex@mbp:~$ triton-keymaster init-cn -d us-west-1 -N 3 <1>
Generating compute node keys... done
Ready for Yubikey or Token for CN 1... ok
Found Yubikey (Yubikey 4 OTP), serial 4a701a, v4.3.1
Setting Yubikey to OTP+CCID mode... done
Remove Yubikey from USB port and re-plug now... ok
Found Yubikey (Yubikey 4 OTP+CCID), serial 4a701a, v4.3.1
Writing keys to Yubikey... done
Ready for Yubikey or Token for CN 2... ok
Found Yubikey (Yubikey 4 OTP), serial 4a701d, v4.3.1
Setting Yubikey to OTP+CCID mode... done
Remove Yubikey from USB port and re-plug now... ok
Found Yubikey (Yubikey 4 OTP+CCID), serial 4a701d, v4.3.1
Writing keys to Yubikey... done
Ready for Yubikey or Token for CN 3... ok
Found Yubikey (Yubikey 4 OTP), serial 4a701e, v4.3.1
Setting Yubikey to OTP+CCID mode... done
Remove Yubikey from USB port and re-plug now... ok
Found Yubikey (Yubikey 4 OTP+CCID), serial 4a701e, v4.3.1
Writing keys to Yubikey... done
Ready to write pieces for backup media sfo-001.
Attach where? [LOCAL/remote/file]
Found sfo-001 in HL-DT-ST DVDRW GX30N RP09 (scsi 1,0,0)
Writing session... 10% 25% 50% 75% 100% done
Ready to write pieces for backup media ord-001.
Attach where? [LOCAL/remote/file]
Found ord-001 in HL-DT-ST DVDRW GX30N RP09 (scsi 1,0,0)
Writing session... 10% 25% 50% 75% 100% done
Ready to write pieces for backup media nyc-001.
Attach where? [LOCAL/remote/file]
Found nyc-001 in HL-DT-ST DVDRW GX30N RP09 (scsi 1,0,0)
Writing session... 10% 25% 50% 75% 100% done
<2>
f120cdf4-9f7d-960d-8f0a-3846ca55accb,VENOSfEgzfSffZYNjwo4RspVrMsE0U4hsV4QUpHornAU6kOAOrxVUwmVtxKVaLNPr6Gakh8izEUUmYSyW5/D9M9wG/JpdyfUcVAUHYUXttNSzht9mA==
08270f43-28c2-57a1-e216c9d68f56af97e,VENOSYJw9DKMJXoeIWydaPVq+X4EEgLBD3PynNYI7XpQnqjmHdx63SrAalcC2vUZY7QJMwWqmIy6LGL4zyC5wlQRs0C8v4ADfVvaFInrAnwQxqNxnA==
0e3bccd9-a92f-f26d-3c4b5ea00042cbfd2,VENOSeO8zZqS/ybTxLXqAAQsv9IE9pfHIlDIFFq2ubEOUjyPmDTWWv4dfkU+FfvEM6/1BMiY8wZB9N8QGDz7mDxsaQcLQWACuN1blZFOW3tdgPgitg==
----
<1> `-d` here is used to supply the short name of a datacenter we set up
    earlier (this automatically chooses the correct backup media and root public
    key for the operation as necessary). The `-N` option is used to generate 3
    compute node keys in one step.
<2> When used in `-N` mode, this command outputs CSV format entries which can
    be copy-pasted either into `cnapi-adm` on the headnode, or into the
    "Paste new CN identities" page in Triton AdminUI.

To place in the new systems, we have also prepared 4 USB flash disks. These have
already been written using `dd` with the USB image. We can insert the root key
and secure boot config into them as follows:

[source,shell]
----
alex@mbp:~$ triton-keymaster update-usb -d us-west-1 -s <1>
Ready for USB flash disk... [CONTINUE/exit]
Found Triton boot image on /dev/disk0 (DTR30G2)
Updating root key and setting secure mode... done
Ready for USB flash disk... [CONTINUE/exit]
Found Triton boot image on /dev/disk0 (DTR30G2)
Updating root key and setting secure mode... done
...
Ready for USB flash disk... [CONTINUE/exit] exit
----
<1> The `-s` option here is used to set the boot configuration to require a
    secured boot process. Fallback to traditional PXE+TFTP with no
    authentication will not be allowed.

notes...

Run through steps required to deploy the whole system from root key to
all CNs up and running

 . Before beginning to set up hardware, run tools (on laptop) to generate root
   key and program Yubikey for initial set of Trusted CNs, including at least
   one headnode. Set up 3 DVD-Rs as backup media (as multi-track UDF), burn
   media key and initial metadata to first track, followed by backups of root
   key and disk encryption secrets for the initial nodes.
 . Run tool to update the USB flash drive image for booting headnodes. Copies
   the root public key into it, as well as the top-level certificates for the
   initial set of headnodes (edits the .img file in one step).
 . Deploy the headnodes for the datacenter, with Yubikeys already present in
   the chassis at first boot.
 . Add CNs (small deployment method)
 .. (If needed) Update the USB flash drive image to be deployed with the current
    set of headnodes' certificates (run the .img updater tool).
 .. Run tools (on laptop) to write a Yubikey for each new CN to be deployed.
    Supply the identities of 3 distinct backup media, and the tool outputs the
    encrypted pieces.
 .. Transport the 3 pieces to the locations of the DVD-Rs set up at the
    beginning and write them in as a new UDF track.
 .. Copy-paste the public keys written to the Yubikey (output by the tool) into
    adminui or a commandline tool on a headnode to establish trust.
 .. Place Yubikey into the new CN and boot.
 . Add CNs (large deployment method with pre-flight)
 .. Add blank Yubikeys into each new CN in the fleet
 .. Boot new CNs to pre-flight. After pre-flight checks are completed, it will
    write the Yubikey, and pass both the public key and the encrypted pieces of
    for backup to the pre-flight controller.
 .. Pre-flight will also write the correct USB image (with certificates added)
    to the USB flash drive in each CN.
 .. Transport the encrypted pieces of each key from the pre-flight controller to
    the location of the backup media and write them out.
 .. Copy the public keys from the pre-flight controller to a headnode to
    establish trust.
 .. Boot the new CNs.

[[brown-field]]
## Brown-field deployment

 * Deploying this on an existing DC

## Implementation and intermediate states

So far, we have described the eventual state of affairs that Triton will be in
after a full implementation of this document. However, the process of
implementation will necessarily involve some intermediate states of development,
which will likely also be deployed to some installations along the way.

Additionally, not all administrators of Triton installations will see fit to
deploy with hardware tokens -- and it may be prohibitively difficult to do so in
some cases -- e.g. deployments within virtual machines for development.

 * Do the USB key and token support stuff first
 * Then soft-token (well, at the same time really)

 * The road to validating everything in the admin vlan, what intermediate states
   will look like while upgrading.
 * What things will look like if you never add any Yubikeys (TLS with just
   self-signed certs, open trust).

## PostgreSQL and Moray

 * Auth and TLS. Using LDAP to validate signatures as passwords?
 * In current version of PostgreSQL, the main limitation for using mTLS for
   AuthN/AuthZ is that PG has not supported reloading of certificates without
   a server restart. PostgreSQL now has certificate reloading on master, not
   yet in PG9.6. Reload is triggered by SIGHUP and/or "pg_ctl reload."
   Backporting a patch to PG9.2 would not be difficult
   (https://github.com/postgres/postgres/commit/de41869b64d57160f58852eab20a27f248188135[postgres change on master].)

[[soft-token]]
## Zone soft token details

The soft token consists of a number of key components:

 * The dedicated HMAC secret for soft token protection, stored in the CN's
   hardware token
 * The soft token key data files, stored encrypted on ZFS within the zone's
   dataset
 * The SSH agent protocol socket, placed as a UNIX socket within the zone's
   filesystem
 * The soft token daemon itself, running within the global zone, and listening
   on the UNIX socket

### Soft token key data

Soft token key data will be stored in the `/zones/$uuid/softhsm` directory.
Each key stored on behalf of the zone will be stored in a separate file,
encrypted (and authenticated) using ChaCha20-Poly1305. The file format will
include the challenge value that must be sent to the hardware token to derive
the symmetric key to decrypt the file, followed by the encrypted data and MAC.

### SSH agent socket

The SSH agent socket for communicating with the soft token will be placed in
the `/.zonecontrol` directory.

The existing `metadata.sock` inside the `zonecontrol` directory currently relies
on the permissions of the enclosing directory to manage access to the metadata
socket. These permissions will be moved to the socket itself, and the
`/.zonecontrol` directory will be world-readable and world-traversable. The
agent socket will use privileges, not filesystem permissions, to manage access.

The socket file itself within `/.zonecontrol` will be named `token.sock` (i.e.
its full path will be `/.zonecontrol/token.sock`). The socket file will be
world-writable and world-readable.

Upon a connection being made by a client process, the soft token daemon will
examine the `cred_t` of the connecting process. Either a new system-wide
privilege bit, `PRIV_ZONE_TOKEN` will be added, or a parametrized privilege will
be implemented, and any connecting process in possession of this privilege will
be allowed to use the soft token.

This privilege will be part of the default zone-wide limit set, but not part of
`basic` or the ordinary user privilege sets. This means that by default, only
root will be able to use the soft token, but end-users can configure their zones
to give this privilege to ordinary users or single processes, and processes can
give up the ability to use the soft-token if they no longer require it (enabling
privilege separation models to be used).

### Soft token daemon

The soft token daemon is started in the global zone as a child of the soft token
manager process. The manager itself is started by SMF.

The top-level manager process' role is to manage the lifecycle of socket files
and lofs-mounting them into zones. Each time it creates a new socket for a
given zone, it forks into a child which handles that zone.

The zone child of the manager is a privileged process whose role centers around
management of key material. It maps dedicated areas of memory (with `MAP_SHARED`
supplied to `mmap()`) for the placement of keys, fills them with the encrypted
key data, and then forks.

This final child is the process which is responsible for speaking the SSH
agent protocol and performing cryptographic operations. It drops all privileges
(including those in the `basic` set) before accepting any connections. To unlock
keys, it sends a single byte request on a pipe back to the key manager process,
which decrypts the keys in-place in the shared memory segment.

#### Performance and accounting

Unlike a regular SSH agent, the soft token daemon final process (serving the
real workload of the zone) will be multi-threaded. Operations will be carried
out by worker threads in a thread pool of limited size. This enables both
pipelining of operations within a single agent connection, and also concurrency
across multiple connections.

Eventually, a mechanism will be used to place the final child process into the
non-global zone for CPU accounting purposes, without making it able to be
traced or debugged by the zone (this will be analogous to a system process in
the global zone).

#### Hardware memory protection

Pending hardware and operating system support, the soft token will support the
use of Intel SGX enclaves (and the analogous features on AMD platforms) to
protect the key data and operating state of the soft token in memory.

This will defend against a variety of attacks on the soft token from other parts
of the system, as well as cold-boot attacks on system memory.

#### Cache side-channel mitigation

On modern Intel CPUs, the soft token will (pending OS support) make use of the
Intel CAT feature to mitigate CPU cache timing side-channel attacks. This will
be done along the lines of the
http://palms.ee.princeton.edu/system/files/CATalyst_vfinal_correct.pdf["CATalyst" paper]
where a special subset of the L3 cache capacity on the system is set aside for
transient use in cryptography, and dedicated pages for this purpose pinned into
cache so they cannot be flushed out.

This prevents most known mechanisms of memory timing side-channel leakage from
the cryptographic algorithms run in the soft-token, including Flush+Reload and
other related attacks. We are also aided here by the fact that KSM (kernel same-
page merging) is not implemented or supported by illumos (and will not be).

As well as this direct mitigation, the algorithms chosen (see the
<<crypto-algos>> section) for soft-token usage are chosen with side-channel
leak prevention in mind.

[[hardware]]
## Hardware implementation

Both the Yubikey and JavaCard USB tokens present a common interface -- the USB
CCID (Chip Card Interface Device) device class. As this (unlike the HID
interfaces on Yubikeys and other devices) is an open interface, with readily
available specifications, this is the interface that is used for the purposes
of this design.

The CCID interface was originally intended for communication between hosts and
smartcards that speak the ISO 7816-4 protocol stack. Even though the USB
devices discussed here are not a smartcard in a card reader, they present
themselves to the host as if they were one. This means that the ISO 7816-4
protocol must be used to communicate with them, just as for a real smartcard.

While the ISO 7816 family of specifications specifies the commands and protocol
used for this communication, as well as some aspects of the data model on
compliant cards, it does not fully specify the structure and organisation of
key material storage.

As a result, additional specifications have arisen to describe the "directory
structure" and missing details of data model for particular applications using
cryptographic smartcards. One of the most commonly known and implemented of
these is the NIST Personal Identity Verification (PIV) standard. This standard
is implemented by both Yubikeys and other JavaCard token manufacturers.

As a result, for asymmetric crypto operations, the interface that the RFD77
implementation uses is PIV over ISO 7816-4 over CCID over USB. We specifically
use the PIV Card Authentication (`0x9E`) key slot, as it does not require a
PIN to perform signing operations.

For the symmetric crypto operation we require from the token (an HMAC), we use
the Yubikey proprietary interface. We also provide open-source code to implement
this interface on regular JavaCards. The Yubikey proprietary interface uses its
own ISO 7816 AID (`a0:00:00:05:27:20:01`), and has a very simple command set
for performing the HMAC operation against a choice of 2 pre-configured keys.

### Operating system infrastructure

Most other open-source operating systems (e.g. GNU/Linux distributions) use a
userland-only suite of software for interacting with CCID smartcards. These are
usually backed by `libusb` or similar (the leading example of such a suite
would probably be OpenSC and pcsclite).

Proprietary operating systems such as Microsoft Windows and the Apple Mac OS
have instead opted to implement fairly deeply integrated smartcard suites
in the operating system base, in order to fully support integration with other
operating system features (e.g. using smartcards seamlessly for user login,
or Windows domain machine authentication etc).

For SmartOS, we propose to implement a hybrid approach similar to the Apple
Mac OS. There will be a deeply integrated operating system component for card
identification and operational use, but card administration and deployment
operations will be handled by software running entirely in userland.

This will allow us to integrate deeply with operating system features such as
the fine-grained privilege model and RBAC, as well as zones. We will provide
a public interface specific to SmartOS (working title `libchipcard`), as well
as implementations of the PCSC API (compatible with `pcsclite` and Mac OS) and
a subset of PKCS#11.

Components built as part of this design (e.g. the soft token, and key provider
for ZFS) are expected to exclusively use the `libchipcard` interface, with the
exception of the deployment and administration tools, which will be largely
based on the PCSC interface (which will also make them largely cross-platform).

The OS infrastructure to be built out here, including the `libchipcard`
interface, will be the subject of a forthcoming RFD specific to their
implementation.

[[crypto-algos]]
## Cryptographic algorithms

One important part of any design involving crytographic primitives is the choice
of algorithms in use. This section is devoted to discussion about options and
trade-offs made in algorithm choice above.

### At-rest encryption

The algorithm to be used for at-rest encryption key derivation is HMAC-SHA1.
This is chosen because:

 * HMAC is well-studied in the context of combining key pieces together in the
   way proposed (combining a key piece from the headnode into a composite key).
   HMAC with a strong hash function has a variety of properties (including
   high diffusion) that make it a good fit for this process.
 * HMAC with SHA-family algorithms is easier to implement in a side-channel
   resistant fashion than other symmetric algorithms such as AES, which is
   important for long-lived hardware keys that are very difficult to change
   (to prevent recovery of the key from a device)
 * The Yubikey already supports HMAC-SHA1 as part of its regular
   challenge-response design. It does not support any SHA2 family algorithms.

### Public-private encryption

The algorithm used for hardware authentication keys is RSA at 2048-bit key
lengths. This is chosen because:

 * RSA is a widely used and well-studied cryptographic algorithm for signing
   and authentication.
 * The 2048-bit key length is chosen as a trade-off between security level and
   performance -- Yubikeys and JavaCards are very slow at computing 4096-bit
   RSA signatures (on the order of hundreds of milliseconds).
 * Alternatives are not well-supported:
   - Ed25519 is not supported in either Yubikeys or JavaCard hardware.
   - ECDSA on NIST P-curves is supported by Yubikeys but not most JavaCard
     hardware options at this time.

RSA in Smartcard devices has a mixed history of side-channel attacks, but modern
hardware has extensive mitigations to lower their impact. The lack of widespread
support for alternatives at the present time is the main limiting factor here.

### Soft token

Soft tokens will support Ed25519 and RSA-4096 for public/private cryptography.
They will also support ChaCha20-Poly1305 for symmetric key operations (with the
key protected on the Yubikey by the same HMAC-SHA1 above).

Ed25519 and RSA-4096 are chosen because:

 * Ed25519's reference implementation is of excellent code quality and readily
   useable for the soft token.
 * Ed25519 is highly side-channel resistant, particularly to CPU cache timing
   side-channels. The soft token must run on the same hardware as customer
   workload, and possibly the workloads of other customers, meaning that
   resistance to side-channel attacks is paramount.
 * RSA is available in addition to Ed25519, as Ed25519 is not yet widely
   supported in TLS and X.509 certificates. The RSA key can only be used for
   signing X.509 certificates as outlined above, and not for general
   authentication.
 * ECDSA has a questionable history with respect to side-channel attacks,
   with many more successful attacks documented than on the other algorithms
   considered, so it was eliminated.

ChaCha20-Poly1305 is chosen because:

 * It is a strong AEAD cipher + MAC combination that has been quite well-studied
   despite being younger than AES.
 * Its implementation is simpler and built from the beginning to support
   authenticated operation, when compared with AES and other families.
 * It is explicitly designed for side-channel resistance. While AES could have
   been chosen, assuming that AES-NI or SSE3 are available, it is desirable to
   not have to require these CPU features for the system to operate safely.


